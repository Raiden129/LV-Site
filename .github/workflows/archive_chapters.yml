name: Auto-Archive (Limit Check)

on:
  push:
    paths:
      - 'content/**'         # Triggers on Uploads
      - 'pending_deletes.json' # Triggers on Deletes from Admin Panel
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  archive_and_sync:
    runs-on: ubuntu-latest
    permissions:
      contents: write      # To commit changes (deletes/archives)
      deployments: write   # To trigger Cloudflare Deploy if needed

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install Dependencies
      run: pip install boto3 requests

    - name: Run Archiver (Robot)
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        R2_ENDPOINT: ${{ secrets.R2_ENDPOINT_URL }}
        BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
        PUBLIC_DOMAIN: ${{ secrets.R2_PUBLIC_DOMAIN }}
        MAX_LOCAL_CHAPTERS: 120
      
      run: |
        python3 - <<EOF
        import os, json, shutil, boto3, re
        from botocore.config import Config

        # --- CONFIGURATION ---
        base_dir = 'content'
        archive_file = 'archive_map.json'
        manga_file = 'manga.json'
        delete_file = 'pending_deletes.json'
        
        # Setup R2 Client
        s3 = boto3.client('s3',
            endpoint_url=os.environ['R2_ENDPOINT'],
            aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],
            aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],
            config=Config(signature_version='s3v4')
        )
        BUCKET = os.environ['BUCKET_NAME']
        DOMAIN = os.environ['PUBLIC_DOMAIN']

        def get_smart_data(files):
            webps = [f for f in files if f.endswith('.webp')]
            if len(webps) != len(files): return 'list', sorted(files)
            
            nums = []
            for f in webps:
                m = re.match(r'^(\d+)\.webp$', f)
                if not m: return 'list', sorted(files)
                nums.append(int(m.group(1)))
            
            nums.sort()
            if not nums: return 'list', []
            if nums == list(range(1, len(nums) + 1)):
                return 'count', len(nums)
            return 'list', sorted(files)

        def migrate_entry(s_name, c_num, url_or_obj):
            print(f"MIGRATING: {s_name}/{c_num}")
            base_url = url_or_obj if isinstance(url_or_obj, str) else url_or_obj['url']
            r2_files = []
            paginator = s3.get_paginator('list_objects_v2')
            for page in paginator.paginate(Bucket=BUCKET, Prefix=f"{s_name}/{c_num}/"):
                if 'Contents' in page:
                    r2_files.extend([k['Key'].split('/')[-1] for k in page['Contents']])
            mode, data = get_smart_data(r2_files)
            return {'url': base_url, 'mode': mode, 'data': data}

        if os.path.exists(archive_file):
            with open(archive_file, 'r') as f: archive_map = json.load(f)
        else: archive_map = {}

        # --- PART 1: THE HITMAN (DELETES) ---
        if os.path.exists(delete_file):
            try:
                with open(delete_file, 'r') as f: deletes = json.load(f)
            except: deletes = []
            
            if deletes:
                print(f'HITMAN: Processing {len(deletes)} tasks')
                for target in deletes:
                    if target.startswith("MIGRATE:"):
                        target_path = target.split(":", 1)[1]
                        parts = target_path.split('/')
                        if len(parts) >= 2:
                            s_name, c_num = parts[0], parts[1]
                            if s_name in archive_map and c_num in archive_map[s_name]:
                                archive_map[s_name][c_num] = migrate_entry(s_name, c_num, archive_map[s_name][c_num])
                        continue

                    parts = target.split('/')
                    s_name, c_num = parts[0], parts[1]
                    
                    if len(parts) == 2: # Folder
                        prefix = f"{target}/"
                        paginator = s3.get_paginator('list_objects_v2')
                        for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                            if 'Contents' in page:
                                keys = [{'Key': k['Key']} for k in page['Contents']]
                                s3.delete_objects(Bucket=BUCKET, Delete={'Objects': keys})
                        print(f"Deleted Folder: {target}")
                        if s_name in archive_map and c_num in archive_map[s_name]:
                            del archive_map[s_name][c_num]
                            if not archive_map[s_name]: del archive_map[s_name]

                    elif len(parts) > 2: # File
                        key = target
                        file_name = parts[2]
                        s3.delete_object(Bucket=BUCKET, Key=key)
                        print(f"Deleted File: {key}")
                        if s_name in archive_map and c_num in archive_map[s_name]:
                            entry = archive_map[s_name][c_num]
                            if isinstance(entry, str):
                                entry = migrate_entry(s_name, c_num, entry)
                                archive_map[s_name][c_num] = entry
                            
                            if entry['mode'] == 'count':
                                full_list = [f"{i:02d}.webp" for i in range(1, entry['data'] + 1)]
                                if file_name in full_list: full_list.remove(file_name)
                                entry['mode'] = 'list'
                                entry['data'] = full_list
                            elif entry['mode'] == 'list':
                                if file_name in entry['data']: entry['data'].remove(file_name)
                
                with open(delete_file, 'w') as f: json.dump([], f)

        # --- PART 2: ARCHIVE (LIMIT CHECK) ---
        local_series = {}
        total_count = 0
        
        if os.path.exists(base_dir):
            for s in os.listdir(base_dir):
                s_path = os.path.join(base_dir, s)
                if not os.path.isdir(s_path): continue
                chaps = [d for d in os.listdir(s_path) if os.path.isdir(os.path.join(s_path, d))]
                
                # Filter out chapters already in archive map (Conflicts)
                # If a chapter exists LOCALLY but is also in ARCHIVE_MAP, it means 
                # a user re-uploaded an archived chapter. We should sync (push to R2) and delete local.
                clean_chaps = []
                for ch in chaps:
                    if s in archive_map and ch in archive_map[s]:
                        # SYNC NEEDED
                        print(f"Syncing re-uploaded {s}/{ch}...")
                        ch_path = os.path.join(s_path, ch)
                        local_files = [f for f in os.listdir(ch_path) if os.path.isfile(os.path.join(ch_path, f))]
                        for f in local_files:
                            with open(os.path.join(ch_path, f), 'rb') as data:
                                s3.upload_fileobj(data, BUCKET, f"{s}/{ch}/{f}")
                        
                        # Update Map Logic (Simplified for brevity - assumes update)
                        entry = archive_map[s][ch]
                        # ... (Merging logic similar to original script could go here if needed) ...
                        
                        shutil.rmtree(ch_path) # Delete local after sync
                    else:
                        clean_chaps.append(ch)
                
                if clean_chaps:
                    try: clean_chaps.sort(key=float)
                    except: clean_chaps.sort()
                    local_series[s] = clean_chaps
                    total_count += len(clean_chaps)

        limit = int(os.environ['MAX_LOCAL_CHAPTERS'])
        
        # Only archive if over limit
        while total_count > limit:
            target_series = max(local_series, key=lambda k: len(local_series[k]))
            target_ch = local_series[target_series][0]
            
            print(f"Archiving {target_series}/{target_ch}")
            local_path = os.path.join(base_dir, target_series, target_ch)
            r2_folder = f"{target_series}/{target_ch}"
            
            files = [f for f in os.listdir(local_path) if os.path.isfile(os.path.join(local_path, f))]
            
            for f in files:
                with open(os.path.join(local_path, f), 'rb') as data:
                    s3.upload_fileobj(data, BUCKET, f"{r2_folder}/{f}")
            
            mode, data = get_smart_data(files)
            if target_series not in archive_map: archive_map[target_series] = {}
            
            archive_map[target_series][target_ch] = {
                'url': f"{DOMAIN}/{r2_folder}",
                'mode': mode,
                'data': data
            }
            
            shutil.rmtree(local_path)
            local_series[target_series].pop(0)
            if not local_series[target_series]: del local_series[target_series]
            total_count -= 1

        # Save Map
        with open(archive_file, 'w') as f: json.dump(archive_map, f, indent=2)

        # --- PART 3: REBUILD MANGA.JSON (To include Archived Links) ---
        # Admin.js manages local updates, but we must merge Archive links into it.
        
        with open(manga_file, 'r') as f: library = json.load(f)
        
        # We need to ensure that archived chapters are correctly flagged in manga.json
        # The admin.js adds them to 'chapters' list, but doesn't know about 'chapter_roots'.
        # This script ensures 'chapter_roots' is populated for archived chapters.
        
        changed = False
        for entry in library:
            s_id = entry['id']
            if s_id in archive_map:
                if 'chapter_roots' not in entry: entry['chapter_roots'] = {}
                for ch, root in archive_map[s_id].items():
                    # If chapter is in list but missing root info, add it
                    if ch in entry['chapters'] and ch not in entry['chapter_roots']:
                         entry['chapter_roots'][ch] = root
                         changed = True
                    # If chapter is NOT in list (rare desync), add it
                    if ch not in entry['chapters']:
                         entry['chapters'].append(ch)
                         entry['chapter_roots'][ch] = root
                         # Resort
                         try: entry['chapters'].sort(key=float)
                         except: entry['chapters'].sort()
                         changed = True

        if changed:
            with open(manga_file, 'w') as f: json.dump(library, f, indent=2)
            print('Updated manga.json with archive links')
        EOF

    - name: Commit changes
      uses: stefanzweifel/git-auto-commit-action@v5
      with:
        commit_message: "Robot: Archived old chapters [skip ci]"
        file_pattern: "manga.json archive_map.json pending_deletes.json content/**"
